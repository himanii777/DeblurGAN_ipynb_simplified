{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKUPJGF9tIze"
      },
      "outputs": [],
      "source": [
        "#============Make necessary imports==========#\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ_m8DsrtIzg",
        "outputId": "60bb58e8-b6c3-4efc-eeb8-42854def2dd0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=========Find the correct resizing dimesion=========#\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We want to decrease the dimension of the images while making all of them consistent. \n",
        "One way to do so is by checking aspect ratio and finding the dimensions which maintains the AR ( we dont want any image distortion)\n",
        "\"\"\"\n",
        "\n",
        "def get_image_aspect_ratios(image_folder):\n",
        "    aspect_ratios = {}\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            with Image.open(image_path) as img:\n",
        "                width, height = img.size\n",
        "                aspect_ratio = width / height\n",
        "                aspect_ratios[filename] = aspect_ratio\n",
        "    return aspect_ratios\n",
        "\n",
        "\n",
        "image_folder = \"/write down the path/\"\n",
        "aspect_ratios = get_image_aspect_ratios(image_folder)\n",
        "\n",
        "for filename, aspect_ratio in aspect_ratios.items():\n",
        "    print(f'{filename}: {aspect_ratio:.2f}')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "In my case its 1.78 and (224, 128) maintains that ratio (almost).\n",
        "You don't have to 100% get the ratio but similar\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qjg_O_EtIzh",
        "outputId": "54b24281-4948-455c-dac9-a57aacff2f40"
      },
      "outputs": [],
      "source": [
        "#============Importing Images============#\n",
        "\n",
        "\"\"\"\n",
        "I used the Go Pro dataset.\n",
        "Link: https://www.kaggle.com/datasets/jishnuparayilshibu/a-curated-list-of-image-deblurring-datasets\n",
        "\"\"\"\n",
        "\n",
        "def load_images_to_tensor(folder_path, max_images=100):\n",
        "    tensor_list = []\n",
        "    image_files = os.listdir(folder_path)\n",
        "    image_files.sort(key=lambda s: s.lower()) # sorting the sequence blurred and sharp\n",
        "\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,128)), \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # normalize for stable training\n",
        "    ])\n",
        "\n",
        "    for img_filename in image_files[:max_images]:\n",
        "        img_path = os.path.join(folder_path, img_filename)\n",
        "        image = Image.open(img_path)\n",
        "        tensor = transform(image).to(device)\n",
        "        tensor_list.append(tensor)\n",
        "\n",
        "    return tensor_list\n",
        "\n",
        "blur_path = r\"/path to your blurred_data\"\n",
        "sharp_path = r\"/path to your sharped_data\"\n",
        "max_images = 1000 # We will use 1000 images for our model\n",
        "\n",
        "motion_blurred_tensors = load_images_to_tensor(blur_path, max_images)\n",
        "sharp_tensors = load_images_to_tensor(sharp_path, max_images)\n",
        "\n",
        "#Simple train test split\n",
        "motion_blurred_train = motion_blurred_tensors[:int(max_images*0.8)]\n",
        "motion_blurred_test = motion_blurred_tensors[int(max_images*0.8):int(max_images)]\n",
        "sharp_tensors_train = sharp_tensors[:int(max_images*0.8)]\n",
        "sharp_tensors_test = sharp_tensors[int(max_images*0.8):int(max_images)]\n",
        "\n",
        "#Stack the tensors\n",
        "motion_blurred_stack = torch.stack(motion_blurred_train)\n",
        "sharp_stack = torch.stack(sharp_tensors_train)\n",
        "motion_blurred_stack_t = torch.stack(motion_blurred_test)\n",
        "sharp_stack_t = torch.stack(sharp_tensors_test)\n",
        "\n",
        "#Make dataset\n",
        "train_dataset = TensorDataset(motion_blurred_stack, sharp_stack)\n",
        "test_dataset = TensorDataset(motion_blurred_stack_t, sharp_stack_t)\n",
        "\n",
        "#Loading the dataloader\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=2)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "print(motion_blurred_stack.shape)  # Expected shape: [num_samples, 3, 224, 128]\n",
        "print(sharp_stack.shape)           # Expected shape: [num_samples, 3, 224, 128]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=============Helper Functions===========#\n",
        "\n",
        "#needed fopr Unet\n",
        "class ImagePool():\n",
        "    def __init__(self, pool_size=50):\n",
        "        self.pool_size = pool_size\n",
        "        if self.pool_size > 0:\n",
        "            self.num_imgs = 0\n",
        "            self.images = []\n",
        "\n",
        "    def query(self, images):\n",
        "        if self.pool_size == 0:\n",
        "            return images\n",
        "        return_images = []\n",
        "        for image in images.data:\n",
        "            image = torch.unsqueeze(image, 0)\n",
        "            if self.num_imgs < self.pool_size:\n",
        "                self.num_imgs = self.num_imgs + 1\n",
        "                self.images.append(image)\n",
        "                return_images.append(image)\n",
        "            else:\n",
        "                p = random.uniform(0, 1)\n",
        "                if p > 0.5:\n",
        "                    random_id = random.randint(0, self.pool_size-1)\n",
        "                    tmp = self.images[random_id].clone()\n",
        "                    self.images[random_id] = image\n",
        "                    return_images.append(tmp)\n",
        "                else:\n",
        "                    return_images.append(image)\n",
        "        return_images = Variable(torch.cat(return_images, 0))\n",
        "        return return_images\n",
        "    \n",
        "\n",
        "#we need to apply initial weights to our model\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.normal_(m.weight, 0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.normal_(m.weight, 1, 0.02)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def tensor2im(image_tensor, imtype=np.uint8):\n",
        "    image_numpy = image_tensor[0].cpu().float().detach().numpy()\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "def get_visuals(real_A, Generator_out, blur_imgs):\n",
        "    sharp_imgs= tensor2im(real_A)\n",
        "    Generator_out = tensor2im(Generator_out)\n",
        "    blur_imgs = tensor2im(blur_imgs)\n",
        "    return OrderedDict([('Blurred_Train', real_A), ('Restored_Train', Generator_out), ('Sharp_Train', blur_imgs)])\n",
        "\n",
        "\n",
        "def parameter_count(net):\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    return num_params\n",
        "\n",
        "#Metrics\n",
        "def mse(img1, img2):\n",
        "    return np.linalg.norm(img1 - img2)\n",
        "\n",
        "def PSNR(img1, img2):\n",
        "\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    pixel_max = 255.0\n",
        "    return 20 * math.log10(pixel_max / math.sqrt(mse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#===============Discriminator==============#\n",
        "\n",
        "\"\"\"\n",
        "For discriminator we are implementing patchGAN\n",
        "Link: https://paperswithcode.com/method/patchgan\n",
        "\"\"\"\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    input_nc : input number of channel\n",
        "    ndf: discriminator's filter\n",
        "    n_layers: convolutional layers\n",
        "    norm: we use instance norm because of smaller batch size\n",
        "    bias: we use bias only for instance norm, not for batch norm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d, use_bias=True):\n",
        "        super(Discriminator, self).__init__()\n",
        "    \n",
        "        \n",
        "        kw = 4\n",
        "        padw = int(np.ceil((kw - 1) / 2)) # so that the output has same spatial dimension as input\n",
        "        layers = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        layers += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        layers += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out= self.model(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#==============ResNet Generator============#\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(\n",
        "            self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.InstanceNorm2d, use_bias=True, use_dropout=False,\n",
        "            n_blocks=9, learn_residual=False, padding_type='reflect'):\n",
        "        \n",
        "        \"\"\"\n",
        "        input_nc: input channels\n",
        "        output_nc: output channels\n",
        "        ngf: number of generator's filters\n",
        "        learn_residual : whether or not we want skip connections\n",
        "        \"\"\"\n",
        "\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "\n",
        "        #Initial Layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
        "            norm_layer(ngf),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "\n",
        "\n",
        "        #Downsample\n",
        "        n_downsampling = 2\n",
        "        layers += [\n",
        "            nn.Conv2d(ngf, ngf*2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
        "            norm_layer(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(ngf*2, ngf*4, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
        "            norm_layer(ngf*4),\n",
        "            nn.ReLU(True)\n",
        "        ]\n",
        "\n",
        "        #Residual blocks        \n",
        "        for i in range(n_blocks):\n",
        "           \n",
        "            layers += [\n",
        "                ResnetBlock(ngf*4, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)\n",
        "            ]\n",
        "\n",
        "        #Upsample\n",
        "        layers += [\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
        "            norm_layer(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
        "            norm_layer(ngf),\n",
        "            nn.ReLU(True),\n",
        "        ]\n",
        "\n",
        "        #Final Layer\n",
        "        layers += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        out= self.model(x)\n",
        "        if self.learn_residual:\n",
        "            out+=x\n",
        "            out = torch.clamp(out, min=-1, max=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "\n",
        "\tdef __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "\t\tsuper(ResnetBlock, self).__init__()\n",
        "\n",
        "\t\tpadAndConv = {\n",
        "\t\t\t'reflect': [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)],\n",
        "\t\t\t'replicate': [\n",
        "                nn.ReplicationPad2d(1),\n",
        "                nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)],\n",
        "\t\t\t'zero': [\n",
        "                nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=use_bias)]\n",
        "\t\t}\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tblocks = padAndConv[padding_type] + [\n",
        "\t\t\t\tnorm_layer(dim),\n",
        "\t\t\t\tnn.ReLU(True)\n",
        "            ] + [\n",
        "\t\t\t\tnn.Dropout(0.5)\n",
        "\t\t\t] if use_dropout else [] + padAndConv[padding_type] + [\n",
        "\t\t\t\tnorm_layer(dim)\n",
        "\t\t\t]\n",
        "\t\texcept:\n",
        "\t\t\traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "\t\tself.conv_block = nn.Sequential(*blocks)\n",
        "\n",
        "\t\t\n",
        "\tdef forward(self, x):\n",
        "\t\tout = x + self.conv_block(x)\n",
        "\t\treturn out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#==========debug code=========#\n",
        "\n",
        "model= ResnetGenerator()\n",
        "x= torch.randn(1,3,224,128)\n",
        "out= model(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=================Unet Generator=================#\n",
        "\n",
        "\"\"\"\n",
        "The original code has also implemented Unet Generator.\n",
        "So choose based on your task and performance\n",
        "\"\"\"\n",
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(\n",
        "            self, input_nc=3, output_nc=3, num_downs=8, ngf=64, norm_layer=nn.BatchNorm2d,\n",
        "            use_dropout=False, learn_residual=False):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "   \n",
        "        self.learn_residual = learn_residual\n",
        "    \n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer,\n",
        "                                                 use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer)\n",
        "\n",
        "        self.model = unet_block\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n",
        "            output = nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
        "        else:\n",
        "            output = self.model(input)\n",
        "        if self.learn_residual:\n",
        "            output = input + output\n",
        "            output = torch.clamp(output, min=-1, max=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self, outer_nc=3, inner_nc=3, submodule=None,\n",
        "            outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        dConv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "        dRelu = nn.LeakyReLU(0.2, True)\n",
        "        dNorm = norm_layer(inner_nc)\n",
        "        uRelu = nn.ReLU(True)\n",
        "        uNorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            uConv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n",
        "            dModel = [dConv]\n",
        "            uModel = [uRelu, uConv, nn.Tanh()]\n",
        "            model = [\n",
        "                dModel,\n",
        "                submodule,\n",
        "                uModel\n",
        "            ]\n",
        "     \n",
        "        elif innermost:\n",
        "            uConv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "            dModel = [dRelu, dConv]\n",
        "            uModel = [uRelu, uConv, uNorm]\n",
        "            model = [\n",
        "                dModel,\n",
        "                uModel\n",
        "            ]\n",
        "     \n",
        "        else:\n",
        "            uConv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
        "            dModel = [dRelu, dConv, dNorm]\n",
        "            uModel = [uRelu, uConv, uNorm]\n",
        "\n",
        "            model = [\n",
        "                dModel,\n",
        "                submodule,\n",
        "                uModel\n",
        "            ]\n",
        "            model += [nn.Dropout(0.5)] if use_dropout else []\n",
        "\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([self.model(x), x], 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC5_0_gvtIzi"
      },
      "outputs": [],
      "source": [
        "#=====================Losses======================#\n",
        "\n",
        "\"\"\"\n",
        "Loss functions simplified:\n",
        "\n",
        "Loss_generator = adversarial_loss_gen + content_loss * c_lambda \n",
        "Loss_discriminator = adversarial_loss_disc + gradient_penalty * g_lambda\n",
        "We use wasserstain loss as the adversarial loss.\n",
        "\n",
        "Typical adversarial loss:\n",
        "adversarial_loss_gen = - Discriminator(generated_deblurred_imgs).mean\n",
        "(It acts like a penalty for generator's loss)\n",
        "adversarial_loss_disc = Discriminator(generated_deblurred_imgs).mean - Discriminator(True_sharp_imgs).mean\n",
        "\n",
        "For content loss, we use L1 loss\n",
        "but for this we use perception loss. \n",
        "\n",
        "c_lambda= 100\n",
        "g_lamda= 10\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Perception_loss paper: https://arxiv.org/abs/1603.08155\n",
        "\"\"\"\n",
        "\n",
        "def perceptual_loss(y_true, y_pred):\n",
        "    conv_3_3_layer = 14\n",
        "    cnn = models.vgg19(pretrained=True).features\n",
        "    cnn = cnn.cuda()\n",
        "    model = nn.Sequential()\n",
        "    model = model.cuda()\n",
        "    for i, layer in enumerate(list(cnn)):\n",
        "        model.add_module(str(i), layer)\n",
        "        if i == conv_3_3_layer:\n",
        "            break\n",
        "    criterion = nn.MSELoss()\n",
        "    fake = model.forward(y_pred)\n",
        "    real = model.forward(y_true)\n",
        "    f_real = real.detach()\n",
        "    loss = criterion(fake, f_real)\n",
        "    return loss\n",
        "\n",
        "\"\"\"\n",
        "Gradient penalty for stable training\n",
        "Link: https://paperswithcode.com/method/wgan-gp \n",
        "\"\"\"\n",
        "\n",
        "def calc_gradient_penalty(netD, real_data, fake_data):\n",
        "    alpha = torch.rand(1, 1)\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "    alpha = alpha.cuda()\n",
        "    \n",
        "    # alpha = torch.rand(real_data.size(0), 1, 1, 1).cuda()    \n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "    interpolates = interpolates.cuda()\n",
        "    interpolates = Variable(interpolates, requires_grad=True)    \n",
        "\n",
        "    disc_interpolates = netD.forward(interpolates)    \n",
        "    gradients = autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    # gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10    # we already multiplied by lambda here\n",
        "    return gradient_penalty\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSFeWOPvtIzj"
      },
      "outputs": [],
      "source": [
        "#==================Train Function ==================#\n",
        "\n",
        "\n",
        "def train(dataset, D, G, optimizer_G, optimizer_D ,bs, n_epoch, d_training, g_training):\n",
        "\n",
        "    d_loss_total = []\n",
        "    g_loss_total = []\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(1, n_epoch+1):\n",
        "        print(\"Running epoch:\", epoch)\n",
        "        start_time_epoch = time.time()\n",
        "        d = 0\n",
        "        g= 0\n",
        "        for i, (blur, sharp) in enumerate(dataset):\n",
        "\n",
        "            steps += bs\n",
        "            blur_imgs = blur.to(device)\n",
        "            sharp_imgs= sharp.to(device)\n",
        "            Generator_out = G.forward(blur_imgs)\n",
        "\n",
        "            \"\"\"\n",
        "            I added a loop for generator too.\n",
        "            Its for the case when discriminator is way stronger and you might wanna train generator frequently to catch up\n",
        "            If the model is collapsing, decrease disc's learning rate/ train the generator more frequently to balance.\n",
        "            There are other ways to fix model collapsing, check: https://www.reddit.com/r/MachineLearning/comments/i085a8/d_best_gan_tricks/\n",
        "            \"\"\"\n",
        "\n",
        "            # =======================Train the discriminator=======================#\n",
        "            for iter in range(d_training):\n",
        "                optimizer_D.zero_grad()\n",
        "\n",
        "                # Discriminator outputs\n",
        "                real_validity = D(sharp_imgs)\n",
        "                fake_validity= D(Generator_out.detach())\n",
        "\n",
        "                # Gradient penalty\n",
        "                gradient_penalty = calc_gradient_penalty(D, sharp_imgs.data, Generator_out.data)\n",
        "                # gradient_penalty=0\n",
        "\n",
        "                d_loss = fake_validity.mean() - real_validity.mean() + gradient_penalty\n",
        "                d_loss.backward(retain_graph=True)\n",
        "\n",
        "                optimizer_D.step()\n",
        "                if iter == d_training-1:\n",
        "                    d += d_loss.item() # add the final loss of iteration\n",
        "\n",
        "        \n",
        "            #========================Train the generator===========================#\n",
        "            for iter in range(g_training):\n",
        "                optimizer_G.zero_grad()\n",
        "\n",
        "                Generator_out = G(blur_imgs)\n",
        "                fake_validity = D(Generator_out)\n",
        "                g_adv_loss = -fake_validity.mean()\n",
        "                g_contentloss = perceptual_loss(Generator_out, sharp_imgs) * 100\n",
        "                g_total_loss = g_adv_loss + g_contentloss\n",
        "                g_total_loss.backward() # no need to retain the graph\n",
        "\n",
        "                \n",
        "                \"\"\"\n",
        "                If training doesn't seem stable, add gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(G.parameters(), max_norm=1.0)\n",
        "                \"\"\"\n",
        "                \n",
        "                optimizer_G.step()\n",
        "                if iter == g_training - 1:\n",
        "                    g += g_total_loss.item()\n",
        "\n",
        "\n",
        "            # pnsr metric\n",
        "            if steps % 1000 == 4:\n",
        "                image_res = get_visuals(sharp_imgs, Generator_out, blur_imgs)\n",
        "                psnr = PSNR(image_res['Restored_Train'], image_res['Sharp_Train'])\n",
        "                print('PSNR on Train (at epoch {0}) = {1}'.format(epoch, psnr))\n",
        "\n",
        "            g+= g_total_loss.item()\n",
        "\n",
        "        d_loss_total.append(d/len(dataset))\n",
        "        g_loss_total.append(g/len(dataset))\n",
        "\n",
        "        #saving model after every 50 epochs\n",
        "        if epoch % 50 == 0:\n",
        "            torch.save(G.state_dict(), 'Generator_' + str(epoch) + '.pt')\n",
        "            torch.save(D.state_dict(), 'Discriminator' + str(epoch) + '.pt')\n",
        "        \n",
        "        end_time_epoch = time.time()\n",
        "\n",
        "        print(\"Time for epoch {0}: {1} | Disc loss: {2}  | Gen loss: {3}\".format(epoch, (end_time_epoch - start_time_epoch), d_loss_total[epoch-1], g_loss_total[epoch-1]))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(d_loss_total, label='Discriminator Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Discriminator Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.savefig('Discriminator_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(g_loss_total, label='Generator Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Generator Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.savefig('Generator_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#==============Define parameters=============#\n",
        "\n",
        "class Parameters:\n",
        "    g_lr = 0.0001\n",
        "    d_lr=0.0001\n",
        "    g_training=1\n",
        "    d_training = 5\n",
        "    beta1 = 0.5 # adam opt constants\n",
        "    beta2 = 0.999\n",
        "    batchsize = 2\n",
        "    n_epochs=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjvh0A_ptIzk",
        "outputId": "8925b7ab-9db4-459e-f53d-3f63fbd21d3d"
      },
      "outputs": [],
      "source": [
        "#==============Start Training==============#\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    D = Discriminator().to(device)\n",
        "    G = ResnetGenerator().to(device)\n",
        "    p= Parameters()\n",
        "    optimizer_G = torch.optim.Adam(G.parameters(), lr=p.g_lr, betas=(p.beta1, p.beta2))\n",
        "    optimizer_D = torch.optim.Adam(D.parameters(), lr=p.d_lr, betas=(p.beta1, p.beta2))\n",
        "    G.apply(init_weights)\n",
        "    D.apply(init_weights)\n",
        "\n",
        "    \n",
        "    p_d= parameter_count(D)\n",
        "    p_g= parameter_count(G)\n",
        "\n",
        "    print('ㅎ ㅡ ㅎ Begin Training ㅎ ㅡ ㅎ')\n",
        "    print(\"Parameters of Discriminator:\",p_d)\n",
        "    print(\"Parameters of Generattor\", p_g)  \n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "\n",
        "    start_time = time.time()\n",
        "    train(train_data_loader, D, G, optimizer_G, optimizer_D, p.batchsize, p.n_epochs, p.d_training, p.g_training)\n",
        "    end_time= time.time()\n",
        "    print(\"Total time for training:\", end_time - start_time)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#==============Predicting Images===============#\n",
        "\n",
        "#Load the trained model \n",
        "generator = ResnetGenerator().to(device)\n",
        "generator.load_state_dict(torch.load('saved_name', map_location=device))\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "\n",
        "    if isinstance (tensor, np.ndarray):\n",
        "        tensor= torch.from_numpy(tensor)\n",
        "        \n",
        "    mean = torch.tensor([0.5, 0.5, 0.5])\n",
        "    std = torch.tensor([0.5, 0.5, 0.5])\n",
        "\n",
        "    tensor = tensor.clone().detach()\n",
        "    if tensor.dim() == 4 and tensor.shape[0] == 1:\n",
        "        tensor = tensor.squeeze(0)\n",
        "\n",
        "    tensor = tensor * std[:, None, None] + mean[:, None, None] # Denormalize\n",
        "    tensor = tensor.permute(1, 2, 0)  # CxHxW -> HxWxC\n",
        "    tensor = torch.clamp(tensor, 0, 1)  # Clamp values to ensure they are between 0 and 1\n",
        "\n",
        "    plt.imshow(tensor)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "generator.eval()\n",
        "indices = list(range(len(test_data_loader.dataset)))\n",
        "# random.shuffle(indices)\n",
        "with torch.no_grad():\n",
        "      for set_num in range(5):\n",
        "            set_indices = indices[set_num * 2: (set_num + 1) * 2]  # Adjust the batch size to your needs\n",
        "\n",
        "            for i, idx in enumerate(set_indices):\n",
        "                blurred_imgs, true_sharp_imgs = test_data_loader.dataset[idx]\n",
        "                blurred_imgs = blurred_imgs.unsqueeze(0).to(device)\n",
        "                true_sharp_imgs = true_sharp_imgs.unsqueeze(0).to(device)\n",
        "                predicted_sharp_imgs = generator(blurred_imgs).to(device)\n",
        "\n",
        "                plt.figure(figsize=(15, 5))\n",
        "\n",
        "                # Blurred images\n",
        "                plt.subplot(1, 3, 1)\n",
        "                imshow(blurred_imgs[0].cpu().numpy(), title=\"Blurred\")\n",
        "\n",
        "                # True sharp images\n",
        "                plt.subplot(1, 3, 2)\n",
        "                imshow(true_sharp_imgs[0].cpu().numpy(), title=\"True Sharp\")\n",
        "\n",
        "                # Predicted sharp images\n",
        "                plt.subplot(1, 3, 3)\n",
        "                imshow(predicted_sharp_imgs[0].cpu().numpy(), title=\"Predicted Sharp\")\n",
        "\n",
        "                plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=======continue training=====#\n",
        "\n",
        "def load_model_checkpoint(generator_path, discriminator_path, G, D):\n",
        "    G.load_state_dict(torch.load(generator_path))\n",
        "    D.load_state_dict(torch.load(discriminator_path))\n",
        "\n",
        "def continue_training(dataset, D, G, bs, start_epoch, n_epoch, device):\n",
        "    \n",
        "    d_loss_total = []\n",
        "    g_loss_total = []\n",
        "    steps = 0\n",
        "    d_training = 5\n",
        "    g_training = 1\n",
        "\n",
        "    for epoch in range(start_epoch, n_epoch+1):\n",
        "        print(\"Running epoch:\", epoch)\n",
        "        start_time_epoch = time.time()\n",
        "        d = 0\n",
        "        g = 0\n",
        "        for i, (blur, sharp) in enumerate(dataset):\n",
        "            steps += bs\n",
        "            blur_imgs = blur.to(device)\n",
        "            sharp_imgs = sharp.to(device)\n",
        "            Generator_out = G(blur_imgs)\n",
        "\n",
        "            # =======================Train the discriminator=======================#\n",
        "            for iter in range(d_training):\n",
        "                optimizer_D.zero_grad()\n",
        "\n",
        "                # Discriminator outputs\n",
        "                real_validity = D.forward(sharp_imgs)\n",
        "                fake_validity = D.forward(Generator_out.detach())\n",
        "\n",
        "                # Gradient penalty\n",
        "                gradient_penalty = calc_gradient_penalty(D, sharp_imgs.data, Generator_out.data)\n",
        "                # gradient_penalty = 0\n",
        "\n",
        "                d_loss = fake_validity.mean() - real_validity.mean() + gradient_penalty\n",
        "                d_loss.backward(retain_graph=True)\n",
        "\n",
        "                optimizer_D.step()\n",
        "                if iter == d_training-1:\n",
        "                    d += d_loss.item()\n",
        "\n",
        "            # ========================Train the generator===========================#\n",
        "            for iter in range(g_training):\n",
        "                optimizer_G.zero_grad()\n",
        "\n",
        "                Generator_out = G(blur_imgs)\n",
        "                fake_validity = D(Generator_out)\n",
        "                g_adv_loss = -fake_validity.mean()\n",
        "                g_contentloss = perceptual_loss(Generator_out, sharp_imgs) * 100\n",
        "                g_total_loss = g_adv_loss + g_contentloss\n",
        "                g_total_loss.backward()\n",
        "                optimizer_G.step()\n",
        "\n",
        "                if iter == g_training - 1:\n",
        "                    g += g_total_loss.item()\n",
        "\n",
        "            # PSNR metrics\n",
        "            if steps % 1000 == 4:\n",
        "                image_res = get_visuals(sharp_imgs, Generator_out, blur_imgs)\n",
        "                psnr = PSNR(image_res['Restored_Train'], image_res['Sharp_Train'])\n",
        "                print('PSNR on Train (at epoch {0}) = {1}'.format(epoch, psnr))\n",
        "\n",
        "            g += g_total_loss.item()\n",
        "\n",
        "        d_loss_total.append(d/len(dataset))\n",
        "        g_loss_total.append(g/len(dataset))\n",
        "  \n",
        "        # saving model after every 50 epochs\n",
        "        if epoch % 50 == 0:\n",
        "            torch.save(G.state_dict(), 'Generator_' + str(epoch) + '.pt')\n",
        "            torch.save(D.state_dict(), 'Discrminator_' + str(epoch) + '.pt')\n",
        "       \n",
        "        end_time_epoch = time.time()\n",
        "\n",
        "        print(\"Time for epoch {0}: {1} | Disc loss: {2}  | Gen loss: {3}\".format(epoch, (end_time_epoch - start_time_epoch), d_loss_total[epoch-1], g_loss_total[epoch-1]))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(d_loss_total, label='Discriminator Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Discriminator Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    # plt.savefig('Discriminator_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(g_loss_total, label='Generator Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Generator Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    # plt.savefig('Generator_loss.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    D = Discriminator().to(device)\n",
        "    G = ResnetGenerator().to(device)\n",
        "    p= Parameters()\n",
        "    optimizer_G = torch.optim.Adam(G.parameters(), lr=p.g_lr, betas=(p.beta1, p.beta2))\n",
        "    optimizer_D = torch.optim.Adam(D.parameters(), lr=p.d_lr, betas=(p.beta1, p.beta2))\n",
        "    \n",
        "    #load the pretrained models\n",
        "    load_model_checkpoint('Generator_trained_100epochs.pt', 'Discriminator_trained_100epochs', G, D)\n",
        "    \n",
        "    #DO not apply init weights\n",
        "\n",
        "    print('ㅠ ㅠ Continue Training ㅠ ㅠ ')\n",
        "    start_time = time.time()\n",
        "    continue_training(train_data_loader, D, G, p.batchsize, start_epoch=101, n_epoch=251, device=device)\n",
        "    end_time= time.time()\n",
        "    print(\"Total time for training:\", end_time- start_time)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
